
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../prerequisites/">
      
      
        <link rel="next" href="../deployment/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.17">
    
    
      
        <title>Workshop - Production LLM apps with Azure AI, Prompt Flow, and VS Code</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.bcfcd587.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#prompt-flow-workshop" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Production LLM apps with Azure AI, Prompt Flow, and VS Code" class="md-header__button md-logo" aria-label="Production LLM apps with Azure AI, Prompt Flow, and VS Code" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Production LLM apps with Azure AI, Prompt Flow, and VS Code
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Workshop
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="pink"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 2c-1.82 0-3.53.5-5 1.35C8 5.08 10 8.3 10 12s-2 6.92-5 8.65C6.47 21.5 8.18 22 10 22a10 10 0 0 0 10-10A10 10 0 0 0 10 2Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="cyan"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Production LLM apps with Azure AI, Prompt Flow, and VS Code" class="md-nav__button md-logo" aria-label="Production LLM apps with Azure AI, Prompt Flow, and VS Code" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Production LLM apps with Azure AI, Prompt Flow, and VS Code
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../prerequisites/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prerequisites
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Workshop
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Workshop
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#create-your-first-prompt-flow" class="md-nav__link">
    <span class="md-ellipsis">
      Create your first Prompt Flow
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#create-a-rag-pattern-app" class="md-nav__link">
    <span class="md-ellipsis">
      Create a RAG pattern app
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-1-retrieve-the-r-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Retrieve, the R in RAG
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 1: Retrieve, the R in RAG">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#retrieving-product-information" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieving product information
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Retrieving product information">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-an-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      What is an embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#querying-the-product-index" class="md-nav__link">
    <span class="md-ellipsis">
      Querying the product index
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#retrieving-customer-information" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieving customer information
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#update-the-prompt-flow" class="md-nav__link">
    <span class="md-ellipsis">
      Update the Prompt Flow
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-tools" class="md-nav__link">
    <span class="md-ellipsis">
      Custom tools
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-2-augmentation-the-a-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Augmentation, the A in RAG
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 2: Augmentation, the A in RAG">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prompt-templating" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt templating
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-3-generation-the-g-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Generation, the G in RAG
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 3: Generation, the G in RAG">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calling-the-llm" class="md-nav__link">
    <span class="md-ellipsis">
      Calling the LLM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prompt-evaluations" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt evaluations
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../deployment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deployment
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../local/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Local
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cheat_sheet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Demo cheat sheet
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#create-your-first-prompt-flow" class="md-nav__link">
    <span class="md-ellipsis">
      Create your first Prompt Flow
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#create-a-rag-pattern-app" class="md-nav__link">
    <span class="md-ellipsis">
      Create a RAG pattern app
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-1-retrieve-the-r-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Retrieve, the R in RAG
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 1: Retrieve, the R in RAG">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#retrieving-product-information" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieving product information
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Retrieving product information">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-an-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      What is an embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#querying-the-product-index" class="md-nav__link">
    <span class="md-ellipsis">
      Querying the product index
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#retrieving-customer-information" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieving customer information
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#update-the-prompt-flow" class="md-nav__link">
    <span class="md-ellipsis">
      Update the Prompt Flow
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-tools" class="md-nav__link">
    <span class="md-ellipsis">
      Custom tools
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-2-augmentation-the-a-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Augmentation, the A in RAG
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 2: Augmentation, the A in RAG">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prompt-templating" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt templating
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-3-generation-the-g-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Generation, the G in RAG
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 3: Generation, the G in RAG">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calling-the-llm" class="md-nav__link">
    <span class="md-ellipsis">
      Calling the LLM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prompt-evaluations" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt evaluations
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="prompt-flow-workshop">Prompt Flow Workshop</h1>
<p>This workshop shows how to use Prompt Flow to create a chatbot that is grounded with product and customer information. You'll learn how to:</p>
<ol>
<li>Create a Prompt Flow.</li>
<li>Create a RAG pattern (Retrieval, Augmentation, Generation) to ground the chatbot with context.</li>
<li>How to evaluate the chatbot performance.</li>
<li>How to test the chatbot locally.</li>
</ol>
<h2 id="create-your-first-prompt-flow">Create your first Prompt Flow</h2>
<p>In this section you'll learn the basics of Prompt Flow with VS Code.</p>
<ol>
<li>
<p>Select the Prompt Flow VS Code extension and review the following tabs: <strong>QUICK ACCESS, FLOWS, TOOLS, BATCH RUN HISTORY, and CONNECTIONS</strong>. Note, the connections were created in the previous section.</p>
<p><a class="glightbox" href="../media/connections.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../media/connections.png" /></a></p>
</li>
<li>
<p>Create a new Prompt Flow by following these steps:</p>
<ol>
<li>From the VS Code <strong>Activity Bar</strong>, select the <strong>Explorer</strong> icon.</li>
<li>Right-click the <strong>workshop</strong> folder.</li>
<li>Select <strong>New flow in this directory</strong>.</li>
<li>Select the <strong>Chat flow with the template</strong>.</li>
<li>Finally, review the generated <code>flow.dag.yaml</code> file.</li>
</ol>
</li>
<li>
<p>Select the <strong>Visual editor</strong> link at the top of the <code>flow.dag.yaml</code> file to open the visual editor. This will show the visual representation of the flow. The visual elements include Prompt Flow <strong>Tools</strong> and Tool <strong>Properties</strong>.</p>
<p><a class="glightbox" href="../media/visual_editor.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../media/visual_editor.png" /></a></p>
</li>
<li>
<p>Experiment with the <strong>DAG Tools</strong> to change the layout.</p>
<p><a class="glightbox" href="../media/dag-settings.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../media/dag-settings.png" /></a></p>
</li>
<li>
<p>Set the Prompt Flow parameters:</p>
<ul>
<li>Select the <strong>inputs</strong> tool in the visual editor, then set the <strong>question</strong> to <code>what tents can you recommend for beginners?</code>.</li>
<li>Select the <strong>Chat</strong> tool and set the <strong>connection</strong> to <code>aoai-connection</code>.</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you forget to set these parameters, the flow may not execute using the <strong>Run All</strong> button. Set the missing parameters and run the flow with the <strong>Debug</strong> option <kbd>F5</kbd>.</p>
</div>
</li>
<li>
<p>Run the flow by selecting <strong>Run All</strong> or <kbd>Shift+F5</kbd>, then select <strong>Run it with Standard Mode</strong>. This will execute the flow and will pass the question to the LLM.</p>
<p><a class="glightbox" href="../media/standard_mode.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="standard mode" src="../media/standard_mode.png" /></a></p>
</li>
<li>
<p>When the flow completes, select the <strong>outputs</strong> tool from the Visual editor and review the Prompt Flow tab to show tokens used and run duration.</p>
<p>Keep in mind that the LLM will provide a response based on the data it was trained with. For now, the response does not have any knowledge of your product or customer information.</p>
<p><a class="glightbox" href="../media/prompt_stats.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../media/prompt_stats.png" /></a></p>
</li>
<li>
<p>Review the <strong>Outputs &amp; Logs</strong> for the <strong>outputs</strong> tool. Select <strong>open in new tab</strong> to review the output from the LLM.</p>
<p><a class="glightbox" href="../media/output_in_tab.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="output in new tab" src="../media/output_in_tab.png" /></a></p>
</li>
<li>
<p>Switch back to the <code>flow.dag.yaml</code> file.</p>
</li>
</ol>
<h2 id="create-a-rag-pattern-app">Create a RAG pattern app</h2>
<p>Next, we'll create a RAG pattern app that will ground the LLM prompt with product and customer information. The steps are as follows:</p>
<ol>
<li><strong>Retrieve</strong>, the R in RAG: Retrieve product and customer information.</li>
<li><strong>Augment</strong>, the A in RAG: Augment the LLM prompt with product and customer information.</li>
<li><strong>Generate</strong>, the G in RAG: Generate a response from the LLM.</li>
</ol>
<h2 id="step-1-retrieve-the-r-in-rag">Step 1: Retrieve, the R in RAG</h2>
<p>In this step there will be two retrievals that will be used to ground the LLM prompt with product information and customer order history.</p>
<ol>
<li>Query the product catalog Azure AI Search service for product information</li>
<li>Query the customer database for customer order history information.</li>
</ol>
<h3 id="retrieving-product-information">Retrieving product information</h3>
<p>To retrieve product information, we'll use Azure AI Search. First, we'll generate an embedding for the question and perform a hybrid keyword and semantic search on the <code>contoso-products</code> index in Azure AI Search.</p>
<h4 id="what-is-an-embedding">What is an embedding</h4>
<p>An embedding is a type of vector that is generated by a machine learning model and has semantic meaning. In this workshop, we'll be using the <strong>text-embedding-3-small</strong> model which generates a 1 x 1536 dimensioned vector.</p>
<h4 id="querying-the-product-index">Querying the product index</h4>
<p>Next, both the question and vector are passed to the AI Search engine. AI Search will use a combination of vector and keyword <a href="https://learn.microsoft.com/azure/search/hybrid-search-overview">hybrid search</a> techniques to return the product catalog results that most closely match the question.</p>
<p><a class="glightbox" href="../media/ai-search-query.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../media/ai-search-query.png" /></a></p>
<h3 id="retrieving-customer-information">Retrieving customer information</h3>
<p>To retrieve customer order history, we'll use a custom <code>customer_lookup</code> tool to query the customer order history database.</p>
<h3 id="update-the-prompt-flow">Update the Prompt Flow</h3>
<p>Follow these steps to retrieve the product and customer information:</p>
<ol>
<li><strong>Close the Visual editor tab to avoid issues with the next steps.</strong></li>
<li>From VS Code, navigate to the <strong>workshop</strong> folder and open the <strong>flow.dag.yaml</strong> file. The file contains the YAML representation of the Prompt Flow.</li>
<li>
<p>Replace the existing flow with the following YAML.</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">$schema</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json</span>
<span class="nt">environment</span><span class="p">:</span>
<span class="w">  </span><span class="nt">python_requirements_txt</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">requirements.txt</span>
<span class="nt">inputs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">chat_history</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">list</span>
<span class="w">    </span><span class="nt">is_chat_history</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="w">  </span><span class="nt">question</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">is_chat_input</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">recommended tents for beginners</span>
<span class="w">  </span><span class="nt">customer_id</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;7&quot;</span>
<span class="nt">outputs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">answer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">reference</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${customer_lookup.output}</span>
<span class="w">    </span><span class="nt">is_chat_output</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">context</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">reference</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${retrieve_documentation.output}</span>
<span class="nt">nodes</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">question_embedding</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">package</span>
<span class="w">    </span><span class="nt">tool</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">promptflow.tools.embedding.embedding</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">connection</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aoai-connection</span>
<span class="w">    </span><span class="nt">deployment_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text-embedding-3-small</span>
<span class="w">    </span><span class="nt">input</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.question}</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">retrieve_documentation</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">code</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../contoso-chat/retrieve_documentation.py</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">question</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.question}</span>
<span class="w">    </span><span class="nt">index_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contoso-products</span>
<span class="w">    </span><span class="nt">embedding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${question_embedding.output}</span>
<span class="w">    </span><span class="nt">search</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contoso-search</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">customer_lookup</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">code</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../contoso-chat/customer_lookup.py</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">customerId</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.customer_id}</span>
<span class="w">    </span><span class="nt">conn</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contoso-cosmos</span>
</code></pre></div>
</li>
<li>
<p>You may have to wait 5 to 10 seconds for the flow to validate.</p>
</li>
</ol>
<h3 id="custom-tools">Custom tools</h3>
<ol>
<li>
<p>Select <strong>Visual editor</strong> from the top of the <strong>flow.dag.yaml</strong> file.</p>
</li>
<li>
<p>Review the <strong>customer_lookup</strong> custom tool, click the link to <strong>open code file</strong>.</p>
<p><a class="glightbox" href="../media/tool-link.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../media/tool-link.png" /></a></p>
</li>
<li>
<p>Set a breakpoint in the <strong>customer_lookup</strong> tool on line 4.</p>
</li>
<li>
<p>Review the code for the <strong>customer_lookup</strong> tool. The tool retrieves customer information using the <code>inputs</code> tool <code>customerId</code> property. You can set the <code>customerId</code> value to any number between 1 and 12.</p>
</li>
<li>
<p>Select <strong>Debug</strong> the <strong>customer_lookup</strong> tool. </p>
<ul>
<li>The prompt flow execution starts and stops at the breakpoint you set in the <strong>retrieve_documentation</strong> tool. </li>
<li>Step through the code or press <kbd>F5</kbd> to continue.</li>
<li>Remove the breakpoint and step through the code or press <kbd>F5</kbd> to continue.</li>
</ul>
<p><a class="glightbox" href="../media/debug-tool.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../media/debug-tool.png" /></a></p>
</li>
</ol>
<h2 id="step-2-augmentation-the-a-in-rag">Step 2: Augmentation, the A in RAG</h2>
<p>In this step you'll learn how to augment the LLM prompt with product and customer information to create a prompt that is grounded with context.</p>
<ol>
<li><strong>Close the Visual editor tab to avoid issues with the next steps.</strong></li>
<li>From VS Code, navigate to the <strong>workshop</strong> folder and open the <strong>flow.dag.yaml</strong> file. The file contains the YAML representation of the Prompt Flow.</li>
<li>
<p>Replace the existing flow with the following YAML.</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">$schema</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json</span>
<span class="nt">environment</span><span class="p">:</span>
<span class="w">  </span><span class="nt">python_requirements_txt</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">requirements.txt</span>
<span class="nt">inputs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">chat_history</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">list</span>
<span class="w">    </span><span class="nt">is_chat_history</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="w">  </span><span class="nt">question</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">is_chat_input</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">recommended tents for beginners</span>
<span class="w">  </span><span class="nt">customer_id</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;7&quot;</span>
<span class="nt">outputs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">answer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">reference</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.question}</span>
<span class="w">    </span><span class="nt">is_chat_output</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">context</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">reference</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${customer_prompt.output}</span>
<span class="nt">nodes</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">question_embedding</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">package</span>
<span class="w">    </span><span class="nt">tool</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">promptflow.tools.embedding.embedding</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">connection</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aoai-connection</span>
<span class="w">    </span><span class="nt">deployment_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text-embedding-3-small</span>
<span class="w">    </span><span class="nt">input</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.question}</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">retrieve_documentation</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">code</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../contoso-chat/retrieve_documentation.py</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">question</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.question}</span>
<span class="w">    </span><span class="nt">index_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contoso-products</span>
<span class="w">    </span><span class="nt">embedding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${question_embedding.output}</span>
<span class="w">    </span><span class="nt">search</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contoso-search</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">customer_lookup</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">code</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../contoso-chat/customer_lookup.py</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">customerId</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.customer_id}</span>
<span class="w">    </span><span class="nt">conn</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contoso-cosmos</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">customer_prompt</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">prompt</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">code</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../contoso-chat/customer_prompt.jinja2</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">documentation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${retrieve_documentation.output}</span>
<span class="w">    </span><span class="nt">customer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${customer_lookup.output}</span>
<span class="w">    </span><span class="nt">history</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.chat_history}</span>
</code></pre></div>
</li>
</ol>
<h3 id="prompt-templating">Prompt templating</h3>
<p>Prompt Flow uses <a href="https://pypi.org/project/Jinja2/">Jinja2</a> a templating language for Python, to format prompts.</p>
<ol>
<li>
<p>To see templating in action, select the link on the <strong>customer_prompt</strong> tool.</p>
<p><a class="glightbox" href="../media/customer_prompt_tool_link.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../media/customer_prompt_tool_link.png" /></a></p>
</li>
<li>
<p>Review the template. The template combines the data from the product catalog and the customer database into a prompt with all the context needed for the LLM.</p>
</li>
<li>
<p>Review the sections on safety, documentation (the product info), previous orders (from the customer lookup), and chat history.</p>
</li>
<li>
<p>Switch back to the prompt flow visual editor.</p>
</li>
<li>
<p>Press <kbd>Shift+F5</kbd> or select <strong>Run all</strong> from the designer to run the complete Prompt Flow.</p>
<!-- <a class="glightbox" href="./media/customer_prompt_start.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="./media/customer_prompt_start.png" /></a> -->
</li>
<li>
<p>When the execution has completed, select the <strong>outputs</strong> tool and <strong>open in new tab</strong> to review the output from the Jinja template.</p>
</li>
</ol>
<h2 id="step-3-generation-the-g-in-rag">Step 3: Generation, the G in RAG</h2>
<p>In this step you'll learn how to generate a response from the LLM using the a prompt grounded with product and customer information.</p>
<ol>
<li><strong>Close the Visual editor tab to avoid issues with the next steps.</strong></li>
<li>From VS Code, navigate to the <strong>workshop</strong> folder and open the <strong>flow.dag.yaml</strong> file. The file contains the YAML representation of the Prompt Flow.</li>
<li>
<p>Replace the existing flow with the following YAML.</p>
<div class="copy highlight"><pre><span></span><code><span class="nt">$schema</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json</span>
<span class="nt">environment</span><span class="p">:</span>
<span class="w">  </span><span class="nt">python_requirements_txt</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">requirements.txt</span>
<span class="nt">inputs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">chat_history</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">list</span>
<span class="w">    </span><span class="nt">is_chat_history</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="w">  </span><span class="nt">question</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">is_chat_input</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">recommended tents for beginners</span>
<span class="w">  </span><span class="nt">customer_id</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;7&quot;</span>
<span class="nt">outputs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">answer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">reference</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${llm_response.output}</span>
<span class="w">    </span><span class="nt">is_chat_output</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">context</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">string</span>
<span class="w">    </span><span class="nt">reference</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${retrieve_documentation.output}</span>
<span class="nt">nodes</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">question_embedding</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">package</span>
<span class="w">    </span><span class="nt">tool</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">promptflow.tools.embedding.embedding</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">connection</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aoai-connection</span>
<span class="w">    </span><span class="nt">deployment_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text-embedding-3-small</span>
<span class="w">    </span><span class="nt">input</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.question}</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">retrieve_documentation</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">code</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../contoso-chat/retrieve_documentation.py</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">question</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.question}</span>
<span class="w">    </span><span class="nt">index_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contoso-products</span>
<span class="w">    </span><span class="nt">embedding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${question_embedding.output}</span>
<span class="w">    </span><span class="nt">search</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contoso-search</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">customer_lookup</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">code</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../contoso-chat/customer_lookup.py</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">customerId</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.customer_id}</span>
<span class="w">    </span><span class="nt">conn</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contoso-cosmos</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">customer_prompt</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">prompt</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">code</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../contoso-chat/customer_prompt.jinja2</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">documentation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${retrieve_documentation.output}</span>
<span class="w">    </span><span class="nt">customer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${customer_lookup.output}</span>
<span class="w">    </span><span class="nt">history</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.chat_history}</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm_response</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">code</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../contoso-chat/llm_response.jinja2</span>
<span class="w">  </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">deployment_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpt-35-turbo</span>
<span class="w">    </span><span class="nt">prompt_text</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${customer_prompt.output}</span>
<span class="w">    </span><span class="nt">question</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${inputs.question}</span>
<span class="w">  </span><span class="nt">connection</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aoai-connection</span>
<span class="w">  </span><span class="nt">api</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">chat</span>
</code></pre></div>
</li>
</ol>
<h3 id="calling-the-llm">Calling the LLM</h3>
<p>Next, the prompt that was generated in the previous step will be passed to the LLM.</p>
<ol>
<li>Switch back to the prompt flow visual editor.</li>
<li>Select the <strong>outputs</strong> tool.</li>
<li>Press <kbd>Shift+F5</kbd> or select <strong>Run all</strong> from the designer to run the complete Prompt Flow.</li>
<li>Review the <strong>outputs</strong> of the prompt flow execution by selecting the <strong>outputs</strong> tool, select <strong>open in new tab</strong>.</li>
<li>
<p>Review the Prompt Flow tab to show tokens used and run duration.</p>
<p><a class="glightbox" href="../media/end-2-end-response.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../media/end-2-end-response.png" /></a></p>
</li>
</ol>
<h2 id="prompt-evaluations">Prompt evaluations</h2>
<p>In this step you'll learn how to evaluate the effectiveness of the chat. The evaluation is done by running the chat against the original contoso-chat prompt flow and using the GPT-4 model to evaluate the chat and score how well it performs. There are several use cases for evals, including CI/CD, A/B testing, and model selection.</p>
<ol>
<li>You should run the <strong>/eval/evaluate-chat-local.ipynb</strong> notebook and review the use of gpt-4 to evaluate the effectiveness of the chat.<ul>
<li>This notebook calls the <code>groundedness</code> Prompt Flow in the <code>eval</code> folder which calls GPT-4 to evaluate the context that was sent to the LLM and the response that was returned.</li>
<li>A groundedness metric is calculated and returned.</li>
</ul>
</li>
<li>Note this demo runs against the original contoso-chat prompt flow, not the one we just built.</li>
</ol>
<!-- ## Test the LLM app locally

Before deploying the LLM app to Azure, it can be useful to run it locally for testing.

1. To run the LLM app locally, select run, then select **Build as a local app**.

    ![](./media/run-as-local-app.png)

1. To start the app, select **start local app**.

    ![](./media/start-the-app.png)

1. A browser page will open, and then you can interact with the LLM app.

1. To exit the app, from the VS Code terminal, press <kbd>ctrl+c</kbd>. -->












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.1e8ae164.min.js"></script>
      
    
  <script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body>
</html>